# -*- coding: utf-8 -*-
"""LLM_Workshop_Day1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sfrxame_wG3aJy9QrwXoxQZpsrMUOEIK
"""

!pip install transformers

# Install the required packages
!pip install diffusers torch

from diffusers import StableDiffusionPipeline
import torch

# Load the pre-trained model
pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float32)
pipe = pipe.to("cuda")  # Use GPU for faster generation

# Generate an image from a text prompt
prompt = "Hyper realistic image of darth vader from star wars, with princess leia, with his legendary red lightsaber"
image = pipe(prompt).images[0]

# Save the generated image
image.save("generated_image.png")

"""## Image Enhancement using Img2Img Pipeline"""

# Install the required packages
!pip install diffusers transformers torch Pillow

from diffusers import StableDiffusionImg2ImgPipeline
from PIL import Image
import torch

# Load the pre-trained img2img model (for CPU)
pipe = StableDiffusionImg2ImgPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
pipe = pipe.to("cpu")  # Using CPU

# Load an initial image (input image for modification) in the correct format
init_image = Image.open("generated_image.png").convert("RGB")  # Ensure it's RGB
init_image = init_image.resize((512, 512))  # Resize to 512x512 pixels

# Define the text prompt
prompt = "Give a more fruity version of this picture"

# Generate variations of the image based on the prompt
images = pipe(prompt=prompt, image=init_image, strength=0.75).images

# Save the output modified image
images[0].save("modified_image_cpu.png")

"""## Depth Estimation using AutoModel"""

from transformers import AutoImageProcessor, AutoModelForDepthEstimation
device = "cuda"
checkpoint = "Intel/zoedepth-nyu-kitti"

image_processor = AutoImageProcessor.from_pretrained(checkpoint)
model = AutoModelForDepthEstimation.from_pretrained(checkpoint).to(device)

from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image

pixel_values = image_processor(image, return_tensors="pt").pixel_values.to(device)
print(pixel_values.shape)

import torch

with torch.no_grad():
    outputs = model(pixel_values)

import numpy as np
import torch.nn.functional as F


predicted_depth = outputs.predicted_depth.unsqueeze(dim=1)

height, width = pixel_values.shape[2:]

height_padding_factor = width_padding_factor = 3
pad_h = int(np.sqrt(height/2) * height_padding_factor)
pad_w = int(np.sqrt(width/2) * width_padding_factor)
# print(pad_h)
# print(pad_w)
# if predicted_depth.shape[-2:] != pixel_values.shape[-2:]:
#    predicted_depth = F.interpolate(predicted_depth, size= (height, width), mode='bicubic', align_corners=False)

# if pad_h > 0:
#   predicted_depth = predicted_depth[:, :, pad_h:-pad_h,:]
# if pad_w > 0:
#   predicted_depth = predicted_depth[:, :, :, pad_w:-pad_w]

# print(predicted_depth.shape)

import matplotlib

def colorize(value, vmin=None, vmax=None, cmap='gray_r', invalid_val=-99, invalid_mask=None, background_color=(128, 128, 128, 255), gamma_corrected=False, value_transform=None):
    """Converts a depth map to a color image.

    Args:
        value (torch.Tensor, numpy.ndarray): Input depth map. Shape: (H, W) or (1, H, W) or (1, 1, H, W). All singular dimensions are squeezed.
        vmin (float, optional): Minimum value in the color map. If None, the 2nd percentile of valid pixels is used. Defaults to None.
        vmax (float, optional): Maximum value in the color map. If None, the 85th percentile of valid pixels is used. Defaults to None.
        cmap (str, optional): Matplotlib colormap to use for visualization. Defaults to 'magma_r'.
        invalid_val (int, optional): Specifies value of invalid pixels that should be colored as 'background_color'. Defaults to -99.
        invalid_mask (numpy.ndarray, optional): Boolean mask indicating invalid regions. If None, the invalid_val is used to determine invalid pixels. Defaults to None.
        background_color (tuple[int], optional): 4-tuple (RGBA) color for invalid pixels. Defaults to (128, 128, 128, 255).
        gamma_corrected (bool, optional): Whether to apply gamma correction to the image. Defaults to False.
        value_transform (Callable, optional): Transformation function to apply to valid pixel values before coloring. Defaults to None.

    Returns:
        numpy.ndarray, dtype - uint8: Colored depth map of shape (H, W, 4), representing RGBA channels.
    """

    # Check if the input is a PyTorch tensor and convert it to a NumPy array.
    if isinstance(value, torch.Tensor):
        value = value.detach().cpu().numpy()

    # Remove any extra singleton dimensions (e.g., (1, 1, H, W) becomes (H, W)).
    value = value.squeeze()

    # If no invalid mask is provided, generate one based on invalid_val.
    # invalid_mask marks pixels that have an invalid value.
    if invalid_mask is None:
        invalid_mask = value == invalid_val

    # Mask for valid pixels (logical negation of invalid_mask).
    mask = np.logical_not(invalid_mask)

    # Normalize the valid pixel values to [0, 1] using the given vmin and vmax.
    # If vmin and vmax are not provided, use the 2nd and 85th percentiles of the valid pixels.
    vmin = np.percentile(value[mask], 2) if vmin is None else vmin
    vmax = np.percentile(value[mask], 85) if vmax is None else vmax

    # If vmin and vmax are different, scale the valid pixels to the range [0, 1].
    if vmin != vmax:
        value = (value - vmin) / (vmax - vmin)  # Scale to [0, 1].
    else:
        # If vmin equals vmax (rare), set all values to 0 to avoid division by zero.
        value = value * 0.

    # Assign NaN (Not a Number) to invalid pixels.
    value[invalid_mask] = np.nan

    # Get the colormap from matplotlib and apply it to the normalized values.
    cmapper = matplotlib.colormaps.get_cmap(cmap)

    # If a transformation function is provided, apply it to the valid pixels.
    if value_transform:
        value = value_transform(value)

    # Map the normalized pixel values to RGBA colors using the colormap.
    # The result is an (H, W, 4) array where the 4 represents RGBA channels.
    value = cmapper(value, bytes=True)  # (H, W, 4) with values in [0, 255].

    # Create the output image (RGBA) and assign the background color to invalid pixels.
    img = value[...]
    img[invalid_mask] = background_color  # Set invalid pixels to background color.

    # Optionally apply gamma correction to the image (raises brightness).
    if gamma_corrected:
        img = img / 255  # Normalize to [0, 1] before applying gamma correction.
        img = np.power(img, 2.2)  # Apply gamma correction.
        img = img * 255  # Rescale to [0, 255].
        img = img.astype(np.uint8)  # Convert back to uint8 format.

    # Return the final colored depth map as an image (H, W, 4).
    return img

# Convert the predicted depth (as a tensor) into a NumPy array and pass it to the colorize function.
result = colorize(predicted_depth.cpu().squeeze().numpy())

# Convert the result (colored image) to a PIL image format and display or save.
Image.fromarray(result)

import transformers
model_name = 'Intel/neural-chat-7b-v3-1'
model = transformers.AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

def generate_response(system_input, user_input):

    # Format the input using the provided template
    prompt = f"### System:\n{system_input}\n### User:\n{user_input}\n### Assistant:\n"

    # Tokenize and encode the prompt
    inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=False)

    # Generate a response
    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the assistant's response
    return response.split("### Assistant:\n")[-1]


# Example usage
system_input = "Become a maths teacher, I want you to explain me step by step the calculations for user input ."
user_input = "10 + 2 + 3 + 4 + 5 -3 + 7"
response = generate_response(system_input, user_input)
print(response)

# expected response
"""
To calculate the sum of 100, 520, and 60, we will follow these steps:

1. Add the first two numbers: 100 + 520
2. Add the result from step 1 to the third number: (100 + 520) + 60

Step 1: Add 100 and 520
100 + 520 = 620

Step 2: Add the result from step 1 to the third number (60)
(620) + 60 = 680

So, the sum of 100, 520, and 60 is 680.
"""

!pip install diffusers
#!pip install accelerate - https://huggingface.co/docs/accelerate/en/quicktour
#!pip install transformers
from diffusers import StableDiffusionLDM3DPipeline

pipe = StableDiffusionLDM3DPipeline.from_pretrained("Intel/ldm3d-pano")
#https://huggingface.co/Intel/ldm3d-pano
# On CPU
pipe.to("cpu")
prompt = "A hyper realistic avatar of a guy riding on a Red honda cbr 650r in leather yellow suit,high detail, high quality,8K,photo realism"
name = "Honda-pano"

output = pipe(
        prompt,
        width=1024,
        height=512,
        guidance_scale=7.0,
        num_inference_steps=50,
    )
print(output)
print(type(output))
rgb_image, depth_image = output.rgb, output.depth
rgb_image[0].save(name+"_ldm3d_rgb.jpg")
depth_image[0].save(name+"_ldm3d_depth.png")

"""## Create Panoramic pics from the model"""

from diffusers import StableDiffusionLDM3DPipeline


pipe = StableDiffusionLDM3DPipeline.from_pretrained("Intel/ldm3d-pano")
#https://huggingface.co/Intel/ldm3d-pano
# On CPU
pipe.to("cpu")
prompt = "A hyper realistic avatar of a guy riding on a Red honda cbr 650r in leather yellow suit,high detail, high quality,8K,photo realism"
name = "Honda-pano"

output = pipe(
        prompt,
        width=1024,
        height=512,
        #Output image width and height
        guidance_scale=7.0, # Higher guidance scale encourages to generate images that are closely linked to the text prompt, usually at the expense of lower image quality.
        num_inference_steps=50,  #Number of denoising steps, more the steps means better the picture (but time taking)
    )
rgb_image, depth_image = output.rgb, output.depth
rgb_image[0].save(name+"_ldm3d_rgb.jpg")
depth_image[0].save(name+"_ldm3d_depth.png")

"""## Make changes to existing image using img2img pipeline by Stable Diffusion"""

# Install the required packages
from diffusers import StableDiffusionImg2ImgPipeline
from PIL import Image
import requests
from io import BytesIO
import torch

# Load the pre-trained img2img model for CPU usage
pipe = StableDiffusionImg2ImgPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
pipe = pipe.to("cpu")  # Set the model to CPU

# Download an image from the web (COCO dataset or any other URL)
url = "http://images.cocodataset.org/val2017/000000039769.jpg"  # Example image from COCO dataset
response = requests.get(url)
init_image = Image.open(BytesIO(response.content)).convert("RGB")  # Convert to RGB
init_image = init_image.resize((512, 512))  # Resize to 512x512 pixels

# Define a general prompt for image variation
prompt = "A variation of this image"

# Generate the image variation
images = pipe(prompt=prompt, image=init_image, strength=0.75).images

# Save the output modified image
images[0].save("image_variation_from_web_cpu.png")

# Display the original and the modified image
init_image.show()  # Original image from URL
images[0].show()   # Generated variation

"""## Multiple language response using LLAMA 38B

---


"""

from transformers import pipeline
import intel_extension_for_pytorch as ipex
pipe = pipeline("text-generation", model="GenVRadmin/llama38bGenZ_Vikas-Merged")

supported_languages = ["Tamil", "Telugu", "Assamese", "Kashmiri", "Punjabi", "Bengali", "Sanskrit",
                      "Malayalam", "Sindhi", "Marathi", "Gujarati", "Kannada", "Odia", "Maithili",
                      "Urdu", "Nepali", "Manipuri", "Dogri", "English", "Arabic", "Santali", "Bodo"]
translated_responses = {}

input_phrase = "Can you speak this language?"
for lang in supported_languages:
    prompt = f"Translate the following phrase to {lang}:\n\nInput Phrase: {input_phrase}"
    translated_text = pipe(prompt, max_length=200, num_return_sequences=1)[0]['generated_text']
    print(f"Translated text type {type(translated_text)}")
    print(f"Translated text type {translated_text.shape}")
    translated_responses[lang] = translated_text

# Display the translated responses
for lang, response in translated_responses.items():
    print(f"{lang}: {response}")

"""# NLU using HF Open Source Models

## DistilBERT
"""

# Initialize the sentiment analysis pipeline with the DistilBERT model
classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased')

# Sample text for classification
text = "You are lovely"

# Perform classification
result = classifier(text)[0]

# Output the result
print(f"Label: {result['label']}, Score: {result['score']}")

"""#Named Entity Recognition
NER models typically operate at the token level, meaning they assign a label to each individual token in a sequence. However, many named entities (such as names, organizations, or locations) span multiple tokens. For example, in the phrase "New York City," the words "New," "York," and "City" might each be identified separately by the model as being part of the same entity. The aggregation_strategy helps to combine these related tokens into a single entity label, rather than treating them as separate entities.

Aggregation Strategies:
* `simple` (used below): This strategy groups together contiguous tokens that have the same entity label. For example, if "New" and "York" are both labeled as LOC (Location), they will be grouped together as a single entity, "New York."
It's the most straightforward way to combine related tokens.
Other Aggregation Strategies:

* `none`: No aggregation is performed, so each token is treated individually, and no attempt is made to group related tokens together.
*`first`: Groups contiguous tokens with the same entity label and retains only the label and score of the first token in the sequence.
*`average`: Groups contiguous tokens with the same entity label and averages the scores for each token.
"max": Groups contiguous tokens and takes the maximum score among them for the final entity.

Why is Aggregation Necessary?
In BERT-based models, input text is first tokenized (often using subword tokenization), so multi-word entities or even single words may be split into multiple tokens. Without aggregation, the output would provide a separate prediction for each token, even if those tokens belong to the same entity.

For instance, in the sentence:
"Barack Obama is the former president of the United States."

The NER model might predict `B-PER` (Beginning of Person) for "Barack" and `I-PER` (Inside of Person) for "Obama." Without aggregation, "Barack" and "Obama" would be treated as two separate entities. With aggregation, they would be combined into the single entity "Barack Obama" with the label `PER`.
"""

# Initialize the NER pipeline with a model that has proper labels
ner = pipeline('ner', model='dbmdz/bert-large-cased-finetuned-conll03-english', aggregation_strategy="simple")

# Sample text for NER
text = "Hugging Face is a company based in New York."

# Perform NER
entities = ner(text)

# Output the entities
for entity in entities:
    print(f"Entity: {entity['word']}, Type: {entity['entity_group']}, Score: {entity['score']}")

"""# Language Translator (English to Deutsch)

"""

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
model = MarianModel.from_pretrained("Helsinki-NLP/opus-mt-en-de")

inputs = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt")
decoder_inputs = tokenizer(
    "<pad> Studien haben gezeigt dass es hilfreich ist einen Hund zu besitzen",
    return_tensors="pt",
    add_special_tokens=False,
)
outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_inputs.input_ids)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)

"""## Text Summarization Pipeline"""

# Load the summarization pipeline
summarizer = pipeline('summarization', model='facebook/bart-large-cnn')

# Define a long text
text = """
Artificial Intelligence (AI) is rapidly advancing and transforming various industries.
One of the most significant advancements in AI is in the field of Natural Language Processing (NLP),
which allows machines to understand and generate human language. Hugging Face has developed a powerful set of tools and models for NLP,
which have been widely adopted by developers and researchers. These models are capable of tasks such as text classification,
named entity recognition, question answering, and summarization.
"""

# Summarize the text
summary = summarizer(text, max_length=50, min_length=25, do_sample=False)

# Output the summary
print(summary[0]['summary_text'])

"""# Image Caption Generator

"""

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Download an image from an online URL
image_url = "https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png"  # Sample image
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))

# Preprocess the image and generate a caption
inputs = processor(images=img, return_tensors="pt")
output = model.generate(**inputs)

# Output the caption
caption = processor.decode(output[0], skip_special_tokens=True)
print("Generated Caption:", caption)